
The script in this directory can be used to process a dump of Wikipedia and store it as plain text. 
This in turn can be used as input for the SpaCy parser to create a corpus resource.

The extraction script is documented at https://github.com/attardi/wikiextractor

A dump of the English Wikipedia can be found here (26G!) 

https://meta.wikimedia.org/wiki/Data_dump_torrents

and a number of alternative sites.

I used enwiki-20170420-pages-meta-current.xml.bz2

so enwiki-.....-pages-meta-current is probably what you need, but of course you can use a more recent version

